{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=KMeansClustering, master=local[*]) created by __init__ at <ipython-input-1-82c6572ada23>:15 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-50069cd97c87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mfilePath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"in/test.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Create spark session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"KMeansClustering\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# SparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    314\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 316\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=KMeansClustering, master=local[*]) created by __init__ at <ipython-input-1-82c6572ada23>:15 "
     ]
    }
   ],
   "source": [
    "# run once\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import time\n",
    "\n",
    "# filePath = \"in/Monday-WorkingHours.pcap_ISCX.csv\"\n",
    "filePath = \"in/test.csv\"\n",
    "# Create spark session\n",
    "sc = SparkContext(appName=\"KMeansClustering\")  # SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 4.9188e+04,  4.0000e+00,  2.0000e+00,  0.0000e+00,  1.2000e+01,\n",
      "        0.0000e+00,  6.0000e+00,  6.0000e+00,  6.0000e+00,  0.0000e+00,\n",
      "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0000e+06,\n",
      "        5.0000e+05,  4.0000e+00,  0.0000e+00,  4.0000e+00,  4.0000e+00,\n",
      "        4.0000e+00,  4.0000e+00,  0.0000e+00,  4.0000e+00,  4.0000e+00,\n",
      "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0000e+01,\n",
      "        0.0000e+00,  5.0000e+05,  0.0000e+00,  6.0000e+00,  6.0000e+00,\n",
      "        6.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        0.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "        0.0000e+00,  0.0000e+00,  9.0000e+00,  6.0000e+00,  0.0000e+00,\n",
      "        4.0000e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        0.0000e+00,  0.0000e+00,  2.0000e+00,  1.2000e+01,  0.0000e+00,\n",
      "        0.0000e+00,  3.2900e+02, -1.0000e+00,  1.0000e+00,  2.0000e+01,\n",
      "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        0.0000e+00,  0.0000e+00,  0.0000e+00])]\n",
      "120\n",
      "1.9850444793701172\n"
     ]
    }
   ],
   "source": [
    "raw_data = sc.textFile(filePath)\n",
    "header = raw_data.first() #extract header\n",
    "raw_data = raw_data.filter(lambda row: row != header)   #filter out header\n",
    "\n",
    "def parseLine(line):\n",
    "    cols = line.split(',')\n",
    "    # label is the last column\n",
    "    label = cols[-1]\n",
    "    \n",
    "    # vector is every column, except the label\n",
    "    vector = cols[:-1]\n",
    "    \n",
    "    # convert each value to float\n",
    "    vector = np.array(vector, dtype=np.float)\n",
    "    \n",
    "    return (label, vector)\n",
    "\n",
    "labelsAndData = raw_data.map(parseLine).cache()\n",
    "\n",
    "# we only need the data, not the label\n",
    "data = labelsAndData.map(lambda row: row[1]).cache()\n",
    "print(data.take(1))\n",
    "# number of connections\n",
    "n = data.count()\n",
    "print(n)\n",
    "\n",
    "t1 = time.time()\n",
    "clusters = KMeans.train(data, k=2)\n",
    "print(time.time() - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
